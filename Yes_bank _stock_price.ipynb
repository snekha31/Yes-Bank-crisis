{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  YES bank\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "- Snekha Balamurali\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective:\n",
        "This project aims to develop a predictive model for forecasting Yes Bank's monthly closing stock price based on historical stock data. Given the significant fluctuations in Yes Bank's stock in 2018, accurate stock price predictions are crucial for investors and financial analysts.\n",
        "\n",
        "Dataset & Features:\n",
        "The dataset has yes bank's stock price between the years 2005-2020\n",
        "\n",
        "The dataset includes monthly stock prices, containing features such as:\n",
        "\n",
        "Open Price -The price at which the stock opened for trading.\n",
        "Close Price- The price at which the stock closed at the end of the trading session.\n",
        "High Price- The highest price reached during the month.\n",
        "Low Price- The lowest price recorded during the month.\n",
        " New features like Rolling Averages & Lag Features were created Capturing past trends for better prediction.\n",
        "Seasonality Factors - Using month-based cyclical transformations to model patterns.\n",
        "\n",
        "Approach & Methodology:\n",
        "\n",
        "Exploratory Data Analysis (EDA): Understanding stock trends, volatility, and seasonality.\n",
        "\n",
        "Feature Engineering: Creating lag variables, rolling averages, and cyclical transformations to enhance predictive power.\n",
        "\n",
        "Model Selection: Implementing and evaluating multiple machine learning models, including ARIMA, Ridge Regression, and K-Nearest Neighbors (KNN), to determine the most effective approach.\n",
        "\n",
        "Model Optimization: Fine-tuning hyperparameters to improve prediction accuracy.\n",
        "\n",
        "Explainability: Using SHAP (SHapley Additive exPlanations) to understand the impact of different features on stock price predictions.\n",
        "\n",
        " This predictive model provides valuable insights for investors, analysts, and financial institutions by improving their ability to anticipate stock movements. By leveraging machine learning techniques, stakeholders can make data-driven investment decisions, optimize trading strategies, and mitigate financial risks."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "do2ysCIUplxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes Bank has experienced significant fluctuations in its stock prices, especially following the fraud case involving its former CEO, Rana Kapoor, in 2018. Understanding and predicting stock price movements is crucial for investors and financial analysts. Given a dataset containing monthly stock prices (including opening, closing, highest, and lowest values), the objective of this project is to develop a predictive model that can accurately forecast the closing price of Yes Bank’s stock for a given month. This will help stakeholders make informed investment decisions by leveraging machine learning and time series analysis techniques**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
   
   
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google .colab  import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "file_path = '/content/drive/My Drive/YesBank_StockPrices.csv'\n",
        "\n",
        "yb_df= pd.read_csv(file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "yb_df.head(20)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print('Number of rows:',yb_df.shape[0])\n",
        "print('Number of columns:',yb_df.shape[1] )"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "yb_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "yb_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"Null values\",yb_df.isnull().sum())\n",
        "print(\"NA values\",yb_df.isna().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Nommissing values\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has the  stock price  of  yes bank for 15 years . The data strats from july 2005 and ends in November 2020. It has 185 rows and 5 columns with variables date, open, high, close. Date is in object data  type. and all  the  other variables are in float datatype. The dataset seems to be free from duplicates,  null values or NA's"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "yb_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "yb_df.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Object\n",
        "Date-  Month and year  \n",
        "\n",
        "Float\n",
        "Open- opening price is the first record at which a stock was traded at the opening of a trading session\n",
        "\n",
        "High- Highest price of a stock in the given  day\n",
        "\n",
        "Low- Lowest price of the stock in the day\n",
        "\n",
        "Close- close Price is the last recorded price at which a stock was traded at the end of a trading session"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in yb_df.columns:\n",
        "    unique_values = yb_df[column].unique()\n",
        "    print(f\"Unique values for {column}:\")\n",
        "    print(unique_values)\n",
        "    print()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "sns.boxplot(data=yb_df[['Open', 'High', 'Low', 'Close']])  # Check for outliers\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Calculating IQR and finding outliers\n",
        "columns_to_check = ['Open', 'High', 'Low', 'Close']\n",
        "for col in columns_to_check:\n",
        "    Q1 = yb_df[col].quantile(0.25)\n",
        "    Q3 = yb_df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = yb_df[(yb_df[col] < lower_bound) | (yb_df[col] > upper_bound)]\n",
        "    print(f'{col}: {len(outliers)} outliers found')"
      ],
      "metadata": {
        "id": "ZAVzZ1vfmPG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?\n",
        "\n",
        "The data set didnt have  any null or Na values. No duplicate was also seen. There are few outliers noted. Sincs this  is a  stock prices data it could actually have some meaning. It might be a natural  fluctuation which is not seen in other dataset. So keeping the outliers for now and may be transform the data later."
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Convert Date to datetime format\n",
        "\n",
        "yb_df['Date'] = pd.to_datetime(yb_df['Date'], format='%b-%y') # Specifying the format\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(yb_df['Date'], yb_df['Close'], label='Close Price', color='blue')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Stock Price (close)')\n",
        "plt.title('Yes Bank closing Price Trend between Jul 2005-Nov 2020')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "\n",
        "fig = px.line(yb_df, x='Date', y='Close', title='Yes Bank closing Price Trend between Jul 2005-Nov 2020')\n",
        "fig.update_traces(line_color='blue')\n",
        "fig.update_layout(xaxis_title='Date', yaxis_title='Stock Price (close)')\n",
        "fig.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eM7-2VFJJM3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line graph shows the trends of the overall stocks in different months and years."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The stock price dropped from 350 (2005) to 0 (2020), indicating a severe, uninterrupted bearish trend over 14 years.\n",
        "\n",
        "Accelerated Crisis in 2018-2020: The close price plummeted from 50 (2018) to 0 (2020), reflecting the bank's collapse during this period, likely due to the RBI-imposed moratorium and bad loan crises.\n",
        "\n",
        "The steady decline, especially the terminal drop to zero, signals complete erosion of market trust and operational viability."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph illustrates Yes Bank's growth phase from 2005 to 2018, followed by a sharp decline post-2018. Identifying such trends helps investors make informed decisions.\n",
        " The model can serve as a risk assessment tool by detecting unusual patterns in stock movements, allowing investors to adjust their portfolios accordingly.\n",
        "\n",
        "While the insights gained from this analysis enable better investment decisions and risk mitigation, they also highlight significant financial risks and governance issues that negatively impacted Yes Bank's stock. Stakeholders must balance opportunities with caution to navigate such volatile market conditions effectively."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "#Convert Date column to datetime\n",
        "yb_df['Date'] = pd.to_datetime(yb_df['Date'], format='%b-%y')\n",
        "\n",
        "# Histogram\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(yb_df['Close'], bins=30, kde=True, color=\"blue\")\n",
        "plt.xlabel(\"Closing Price\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Closing Prices\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram tells us about how spread a the data is.It aslo tells us how frequent a closing price occurs in the overall data."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram ranges from 0-350 (closing price).Right-skewed distribution: The majority of closing prices are clustered at the lower end (0-50), with rare spikes up to 350.The presence of extreme outliers (250-350) suggests significant price fluctuations.Over 25% of the data points fall in the 0-50 range, indicating prolonged periods of low stock valuation. The data seems to br right skewed"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram of Yes Bank's closing prices reveals both potential opportunities and significant risks. On the positive side, the occasional extreme spikes in prices (e.g., 250-350 range) suggest short-term volatility that traders could exploit it for quick gains. However, these opportunities are inherently high-risk, as the rarity of such spikes underscores their unpredictability. Conversely, the chart highlights a dominant negative trend: the overwhelming concentration of closing prices in the 0-50 range signals prolonged periods of low valuation, likely tied to systemic issues such as Yes Bank's 2020 financial crisis and subsequent erosion of investor trust. The right-skewed distribution further reinforces that any price recoveries were temporary and insufficient to counteract the long-term downward trajectory.\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=yb_df['Open'], y=yb_df['Close'], alpha=0.7)\n",
        "plt.xlabel(\"Opening Price\")\n",
        "plt.ylabel(\"Closing Price\")\n",
        "plt.title(\"Opening vs Closing Prices\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plots helps to analyze the relationship between opening and closing prices, identify trends in daily price movements, and assess volatility."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both opening and closing prices span 0–350, highlighting high volatility and significant price swings within trading periods.\n",
        "The wide dispersion of prices indicates inconsistent investor behavior, with no clear consolidation around a specific range."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traders could exploit intraday volatility by adopting strategies like buying at low openings and selling at high closings (if the inverse relationship holds). This might yield short-term profits in a momentum-driven market. If stocks often open high (e.g.250-350) but close much lower, it may indicate panic selling.\n",
        "Large price swings (0-350 range) show instability, which can scare off cautious investors.\n",
        "If high openings don't lead to sustained growth, it might reflect failed recovery attempts—similar to Yes Bank's struggles after 2020"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "\n",
        "\n",
        "# Convert Date column to datetime format\n",
        "yb_df['Date'] = pd.to_datetime(yb_df['Date'], format='%b-%y', errors='coerce')\n",
        "\n",
        "# Set Date as index and sort values\n",
        "yb_df = yb_df.sort_values(\"Date\")\n",
        "yb_df.set_index(\"Date\", inplace=True)\n",
        "\n",
        "# Perform seasonal decomposition on the 'Close' price\n",
        "decomposition = seasonal_decompose(yb_df['Close'], model='additive', period=12)\n",
        "\n",
        "# Plot the decomposition\n",
        "decomposition.plot()\n",
        "plt.suptitle(\"Seasonal Decomposition of Closing Prices\", fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A seasonal plot helps visualize trends over time, especially if there are recurring patterns in stock prices."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trend Component:\n",
        "\n",
        "The overall trend of closing prices is increasing over time.\n",
        "However, there are fluctuations in the trend, with an average change of about 4.19 units per time step, indicating moderate volatility.\n",
        "\n",
        "Seasonal Component:\n",
        "\n",
        "The stock price exhibits a seasonal pattern. This suggests that prices tend to rise and fall at certain periods within the year.\n",
        "\n",
        "Residual Component :\n",
        "\n",
        "The residuals have a high standard deviation indicating significant price fluctuations beyond trend and seasonality.\n",
        "This suggests that external factors (e.g., market news, investor sentiment) might be affecting prices unexpectedly."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impact: The increasing trend suggests potential long-term growth, making the stock appealing to long-term investors.\n",
        "\n",
        "Negative Impact: High residual volatility and seasonal dips could signal risk for short-term investors, making it difficult to predict short-term price movements."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "yb_df.reset_index(inplace=True)  # Moves Date back to a column\n",
        "\n",
        "yb_df['Close_MA'] = yb_df['Close'].rolling(window=30).mean()\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(yb_df['Date'], yb_df['Close'], label='Closing Price', alpha=0.5)\n",
        "plt.plot(yb_df['Date'], yb_df['Close_MA'], label='30-Day Moving Average', color='red')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Stock Price')\n",
        "plt.title('Closing Price with 30-Day Moving Average')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A moving average smooths out short-term fluctuations and highlights longer-term trends in stock prices.\n",
        "The 30-day moving average helps identify trends and potential reversals in stock price movements."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From 2006 to around 2016, the stock shows a general upward trend, with the closing price and moving average steadily increasing. This indicates a period of growth and investor confidence.\n",
        "\n",
        "After 2016, particularly around 2018-2020, there is a sharp decline in both the closing price and the moving average.The closing price line shows significant volatility, especially during the period of decline (2018-2020). This indicates high market uncertainty.The 30-day moving average lags behind the closing price during volatile periods, which is expected, but it still captures the overall downward trend. The stock reaches its peak around 2016-2017, with the closing price and moving average at their highest levels.  "
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The moving average helps identify long-term trends, which can guide investment decisions. For example, during 2006-2016 investors could have capitalized on the upward trend.\n",
        "\n",
        "During periods of high volatility, the moving average may give false signals, leading to poor trading decisions. For example, minor recoveries in 2020 might have misled investors into thinking the stock was rebounding, only for it to drop again."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "# Convert Date column to datetime format\n",
        "yb_df['Date'] = pd.to_datetime(yb_df['Date'], format='%b-%y', errors='coerce')\n",
        "\n",
        "# Set Date as index and sort values\n",
        "yb_df = yb_df.sort_values(\"Date\")\n",
        "yb_df.set_index(\"Date\", inplace=True)\n",
        "\n",
        "# Perform seasonal decomposition on the 'Close' price\n",
        "decomposition = seasonal_decompose(yb_df['Open'], model='additive', period=12)\n",
        "\n",
        "# Plot the decomposition\n",
        "decomposition.plot()\n",
        "plt.suptitle(\"Seasonal Decomposition of Opening Prices\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A  seasonal plot helps  visualize trends overtime to findif there  is a recurring pattern."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trend Component:\n",
        "\n",
        "The trend shows a general decline over time, with a few minor recoveries. This suggests that the stock's opening price experienced long-term depreciation.\n",
        "\n",
        "Seasonal Component:\n",
        "\n",
        "There are recurring seasonal patterns where the opening price fluctuates cyclically. Peaks and troughs appear at regular intervals, indicating periodic market influences, such as earnings reports, investor sentiment, or macroeconomic events.\n",
        "\n",
        "Residual Component:\n",
        "\n",
        "The residual (random noise) fluctuates significantly, meaning that while trends and seasonality exist, external factors  contribute unpredictably to price changes"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying a long-term declining trend in the opening price can help businesses adjust their strategies, such as risk management and investment timing. If specific months consistently show an increase or decrease in stock prices, investors can leverage this knowledge for strategic trading.\n",
        "\n",
        "The trend shows a downward movement in opening prices, suggesting declining investor confidence in the stock over time.Large fluctuations in the residual component suggest uncertainty and lack of stability, which may deter risk-averse investors."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[go.Candlestick(x=yb_df.index,\n",
        "                                     open=yb_df['Open'],\n",
        "                                     high=yb_df['High'],\n",
        "                                     low=yb_df['Low'],\n",
        "                                     close=yb_df['Close'])])\n",
        "fig.update_layout(title=\"Yes Bank Candlestick Chart\",\n",
        "                  xaxis_title=\"Date\",\n",
        "                  yaxis_title=\"Stock Price\",\n",
        "                  xaxis_rangeslider_visible=False)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A candlestick chart provides a more detailed view of stock price movements, showing open, high, low, and close prices.\n",
        "It helps in identifying bullish and bearish trends, reversal patterns, and potential trading signals.\n",
        "This is crucial for traders and investors who rely on technical analysis."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The stock saw significant growth between 2014 and 2018, followed by a sharp decline after 2018.\n",
        "The steep fall suggests a market crash, financial distress, or regulatory intervention affecting Yes Bank.\n",
        "The long red candlesticks during the decline indicate strong selling pressure, while the smaller green candles show limited recovery efforts."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The candlestick chart can guide investors on when to buy or sell based on technical signals.\n",
        "Businesses can use this analysis for risk management, investment strategies, and forecasting future trends.The sharp decline in the candlestick chart suggests a financial crisis, governance issues, or regulatory actions."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "yb_df['High_Low_Spread'] = yb_df['High'] - yb_df['Low']\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(yb_df.index, yb_df['High_Low_Spread'], color='red', label=\"High-Low Spread\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price Spread\")\n",
        "plt.title(\"Stock Volatility (High - Low Spread)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart was chosen to analyze the volatility of the stock. The difference between the high and low prices (spread) reflects the level of price fluctuation over time. A larger spread indicates higher volatility.\n",
        "This visualization helps in understanding trends, potential risks, and market sentiment."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From 2018 to 2020, volatility significantly increased, peaking around 2018-2019.\n",
        "The highest volatility periods may indicate major market events, financial instability, or company-specific issues.\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights can be valuable:\n",
        "Understanding volatility trends helps investors make informed decisions, avoiding risky periods.Businesses can use this analysis for risk management, investment strategies, and forecasting future trends.The high volatility during the decline indicates panic selling, loss of investor confidence, and market instability."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = yb_df.corr()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(yb_df[['Open', 'High', 'Low', 'Close']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It shows relationship between different stock variables"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heat map shows high correlation between all the variables. This is expected as Closing price is always closely linked to the Opening price.\n",
        "High and Low prices occur within the same daily range. The high correlation suggests a strong dependency between stock price metrics, meaning that tracking just one variable (e.g., Close Price) is enough to represent the trend. Before building a predictive model, redundant features should be removed to avoid overfitting and multicollinearity."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yb_df.columns"
      ],
      "metadata": {
        "id": "Fd52qg8zhqFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# Create a Pairplot for Open, High, Low, Close\n",
        "yb_df=yb_df.drop(['Close_MA','High_Low_Spread'],axis=1)\n",
        "sns.pairplot(yb_df)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pairplot was chosen because it helps visualize the relationships between multiple numerical variables (Open, High, Low, Close). This type of plot provides scatter plots for pairwise variable comparisons and histograms for individual distributions, making it useful for identifying correlations, trends, and potential outliers."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Open, High, Low, and Close prices show a strong positive correlation, meaning they tend to increase together.\n",
        "The scatter plots display linear relationships, indicating that if one value rises, the others likely do as well.\n",
        "The histograms suggest that the price data may be right-skewed, meaning there are more lower values and fewer higher values.\n",
        "There are a few potential outliers, visible as points deviating from the trend in the scatter plots."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yb_df.reset_index(inplace=True)\n"
      ],
      "metadata": {
        "id": "Lmid_0FFULry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yb_df.columns"
      ],
      "metadata": {
        "id": "BcV1G2D2XP30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H0: The mean opening price is equal to the mean closing price.\n",
        "\n",
        "H1: The mean opening price is significantly different from the mean closing price."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for normality of the  data\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "# Check normality for Open Prices\n",
        "stat_open, p_open = shapiro(yb_df['Open'])\n",
        "print(f\"Shapiro-Wilk Test for Open Price: Statistic={stat_open:.4f}, p-value={p_open:.4f}\")\n",
        "\n",
        "# Check normality for Close Prices\n",
        "stat_close, p_close = shapiro(yb_df['Close'])\n",
        "print(f\"Shapiro-Wilk Test for Close Price: Statistic={stat_close:.4f}, p-value={p_close:.4f}\")\n",
        "\n",
        "# Visual check with Histogram\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(yb_df['Open'], bins=30, alpha=0.7, label=\"Open Price\")\n",
        "plt.hist(yb_df['Close'], bins=30, alpha=0.7, label=\"Close Price\")\n",
        "plt.title(\"Histogram of Open & Close Prices\")\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "oBbTSHFRMRLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Since the  data is  not normal using wilcoxon ranksum test\n",
        "from scipy.stats import wilcoxon\n",
        "\n",
        "# Perform Wilcoxon Signed-Rank Test\n",
        "stat, p = wilcoxon(yb_df['Open'], yb_df['Close'])\n",
        "print(f\"Wilcoxon Test: statistic = {stat:.4f}, p-value = {p:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05\n",
        "if p < alpha:\n",
        "    print(\"Reject Null Hypothesis: There is a significant difference between Open and Close prices.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: No significant difference between Open and Close prices.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "after checking for the normality of both open and  cclose price we figured out  that it  is not normal. planned tomove  with a non parametric test wilcoxon rank sum test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Wilcoxon test does not assume normality, making it ideal for non-normally distributed data.\n",
        "It tests whether the median of the differences between Open and Close prices is significantly different from zero.\n",
        "It is used when comparing two related samples (e.g., Open and Close prices of a stock on the same day/month).\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H0: There is no significant correlation between the high and low prices.\n",
        "\n",
        "H1: There is a significant correlation between the high and low prices."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import shapiro, pearsonr, spearmanr\n",
        "\n",
        "# Check normality for High prices\n",
        "stat_high, p_high = shapiro(yb_df['High'])\n",
        "print(f\"Shapiro-Wilk Test for High Price: Statistic={stat_high:.4f}, p-value={p_high:.4f}\")\n",
        "\n",
        "# Check normality for Low prices\n",
        "stat_low, p_low = shapiro(yb_df['Low'])\n",
        "print(f\"Shapiro-Wilk Test for Low Price: Statistic={stat_low:.4f}, p-value={p_low:.4f}\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Perform Spearman correlation test\n",
        "corr_coeff, p_value = spearmanr(yb_df['High'], yb_df['Low'])\n",
        "\n",
        "print(f\"Spearman Correlation Test: Correlation Coefficient={corr_coeff:.4f}, p-value={p_value:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject H0: There is a significant correlation between High and Low prices.\")\n",
        "else:\n",
        "    print(\"Fail to Reject H0: No significant correlation between High and Low prices.\")\n"
      ],
      "metadata": {
        "id": "QA4mTbEmOzug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have  used spearman correlation test  to find the p-value."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In stock price data, the relationship between High and Low prices may not be linear but still follow a monotonic trend (i.e., as one increases, the other tends to increase).\n",
        "Spearman correlation measures rank-based relationships, making it a better fit for stock price movements, which can be volatile and non-linear. Also it isless sensitive to outliers."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yb_df.head()"
      ],
      "metadata": {
        "id": "K91tYqdA7Nk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NO missing values"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers  are pressent. Since it is a stock  price dataset the extreme values present may represent  a trend which might be useful. Not removing those values."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No textual data presen. So no  categorical encoding"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "#This captures the daily price fluctuation, which is crucial for understanding stock movement.\n",
        "#Since High and Low are highly correlated, keeping both is redundant. Instead, using volatility preserves their key information in one feature\n",
        "yb_df['Volatility'] = yb_df['High'] - yb_df['Low']\n",
        "\n",
        "#This helps determine if the stock price increased or decreased throughout the day.\n",
        "#It captures market in a single number rather than looking at Open and Close separately.\n",
        "yb_df['Price Change'] = yb_df['Close'] - yb_df['Open']\n",
        "\n",
        "# short-term moving average smooths out daily fluctuations and reveals trends.\n",
        "#It helps understand if the stock is trending up or down over the past week.\n",
        "yb_df['Rolling Mean_7'] = yb_df['Close'].rolling(window=7).mean()\n",
        "\n",
        "\n",
        "yb_df['Close_lag1'] = yb_df['Close'].shift(1)\n",
        "yb_df['Rolling_Mean_lag1'] = yb_df['Rolling Mean_7'].shift(1)\n",
        "yb_df.dropna(inplace=True)  # Drop first row due to NaN\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yb_df.describe().T"
      ],
      "metadata": {
        "id": "6CcDy1TWZ0dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# RFE\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "# Convert Date to datetime format (if not already)\n",
        "yb_df['Date'] = pd.to_datetime(yb_df['Date'])\n",
        "\n",
        "# Extract only Year and Month\n",
        "yb_df['Year'] = yb_df['Date'].dt.year\n",
        "yb_df['Month'] = yb_df['Date'].dt.month\n",
        "\n",
        "# Define features and target\n",
        "X = yb_df.drop(columns=['Close', 'Date'])  # Only keeping Year and Month as date-related features\n",
        "y = yb_df['Close']\n",
        "\n",
        "# Initialize RandomForest for RFE\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rfe = RFE(estimator=rf_model, n_features_to_select=5)  # Keep top 5 features\n",
        "\n",
        "# Fit RFE\n",
        "X_selected = rfe.fit_transform(X, y)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features = X.columns[rfe.support_]\n",
        "print(\"Selected features RFE:\", selected_features)\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using random forest to  find the feature importance\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Convert Date to datetime format (if not already)\n",
        "yb_df['Date'] = pd.to_datetime(yb_df['Date'])\n",
        "\n",
        "# Extract only Year and Month\n",
        "yb_df['Year'] = yb_df['Date'].dt.year\n",
        "yb_df['Month'] = yb_df['Date'].dt.month\n",
        "\n",
        "# Define features and target\n",
        "X = yb_df.drop(columns=['Close', 'Date'])  # Only keeping relevant features\n",
        "y = yb_df['Close']\n",
        "\n",
        "# Initialize and fit RandomForest\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X, y)\n",
        "\n",
        "# Get feature importance\n",
        "importances = rf_model.feature_importances_\n",
        "feature_importance = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=feature_importance, y=feature_importance.index)\n",
        "plt.title(\"Feature Importance (Random Forest)\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3SfPRbJxdcJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = yb_df.corr().abs()\n",
        "\n",
        "# Find highly correlated features (correlation > 0.85)\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "high_corr_features = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
        "\n",
        "print(\"Highly correlated features:\", high_corr_features)\n",
        "\n"
      ],
      "metadata": {
        "id": "fWODejkYdrfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking  for multicollinearity\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Define selected features\n",
        "selected_features = [\"Open\",\"Rolling Mean_7\",\"Close_lag1\",\"Low\"]\n",
        "\n",
        "# Create a new DataFrame with only the selected features\n",
        "X = yb_df[selected_features]\n",
        "\n",
        "print(X.isna().sum())  # Count NaNs\n",
        "print((X == np.inf).sum())\n",
        "\n",
        "# Remove any rows with NaN or infinite values\n",
        "X = X.replace([np.inf, -np.inf], np.nan)  # Convert infinities to NaNs\n",
        "X = X.dropna()\n",
        "\n",
        "# Add a constant for VIF calculation\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Compute VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "# Display VIF results\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "id": "V7inWVMseqw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recursive Feature Elimination (RFE)\n",
        "RFE recursively removes less important features while training a model to find the best subset. It selected ['Open', 'High', 'Low', 'Close_lag1', 'Rolling Mean_lag1'] as important.  \n",
        "\n",
        "Feature Importance from Random Forest\n",
        "Random Forest assigns importance scores to each feature, helping identify key predictors.The top features were ['Low', 'High', 'Open', 'Close_lag1', 'Price Change'].\n",
        "\n",
        "Correlation Analysis\n",
        " Highly correlated features can introduce redundancy and instability in the model.['High', 'Low', 'Close', 'Rolling Mean_7'] showed high correlation, suggesting some features may be removed.\n",
        "\n",
        "Variance Inflation Factor (VIF) Multicollinearity Check\n",
        "VIF measures how much a feature is correlated with others, helping to remove multicollinearity.\n",
        "After multiple attempts, we found that Open, Rolling Mean_7, Close_lag1, and Low had relatively low collinearity, making them the most suitable for modeling.\n",
        "\n",
        "By using a combination of these methods, we ensured that the selected features were both important for prediction and did not introduce redundancy or multicollinearity."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Convert month to radians (to handle cyclic nature)\n",
        "yb_df[\"Month_sin\"] = np.sin(2 * np.pi * yb_df[\"Month\"] / 12)\n",
        "yb_df[\"Month_cos\"] = np.cos(2 * np.pi * yb_df[\"Month\"] / 12)\n"
      ],
      "metadata": {
        "id": "0EvJkz6FN5bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "yb_df[\"Year_scaled\"] = scaler.fit_transform(yb_df[[\"Year\"]])\n"
      ],
      "metadata": {
        "id": "_uLlWh91OGkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yb_df.columns"
      ],
      "metadata": {
        "id": "9wY8lvisOggh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "yb_df1= yb_df[[\"Open\", \"Low\", \"Rolling Mean_7\", \"Close_lag1\", \"Month_sin\", \"Month_cos\", \"Year_scaled\",\"Close\"]]\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yb_df1.describe().T"
      ],
      "metadata": {
        "id": "sMxyctkjn0F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew\n",
        "print(yb_df1[['Open', 'Close', 'Rolling Mean_7', 'Close_lag1','Month_sin','Month_cos','Year_scaled' ,'Low']].skew()) # Printing skew\n"
      ],
      "metadata": {
        "id": "ytoE6BzqoZrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from scipy.stats import boxcox\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# List of features for Box-Cox transformation (only positive values, EXCLUDING Close)\n",
        "boxcox_features = [\"Open\", \"Low\", \"Rolling Mean_7\", \"Close_lag1\"]\n",
        "\n",
        "\n",
        "# Create a copy of the original DataFrame\n",
        "yb_df_transformed = yb_df1.copy()\n",
        "\n",
        "# Dictionary to store Box-Cox lambda values\n",
        "lambda_dict = {}\n",
        "\n",
        "# Apply Box-Cox transformation\n",
        "for feature in boxcox_features:\n",
        "    if (yb_df_transformed[feature] > 0).all():  # Ensure all values are positive\n",
        "        transformed_data, lambda_ = boxcox(yb_df_transformed[feature])\n",
        "        yb_df_transformed[feature] = transformed_data\n",
        "        lambda_dict[feature] = lambda_  # Store lambda value\n",
        "        print(f\"Box-Cox Lambda for {feature}: {lambda_}\")\n",
        "    else:\n",
        "        print(f\"Skipping {feature} due to non-positive values.\")\n",
        "\n",
        "\n",
        "\n",
        "# Check skewness after transformation\n",
        "print(\"Skewness after transformation:\")\n",
        "print(yb_df_transformed[boxcox_features] .skew())\n"
      ],
      "metadata": {
        "id": "76_O84cyo25K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Month variable is cyclical (January → December → January).\n",
        "If treated as a normal numerical variable (1 to 12), the model may assume that December (12) is far from January (1), which is incorrect.\n",
        "Converting months into sine and cosine values helps represent their cyclic behavior properly.Box-Cox transformation was applied to reduce skewness (for positive variables).\n"
      ],
      "metadata": {
        "id": "_DXtmuq_rjQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# List of features to scale (EXCLUDING Close)\n",
        "scaled_features = [\"Open\", \"Low\", \"Rolling Mean_7\", \"Close_lag1\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "yb_df_scaled = yb_df_transformed.copy()\n",
        "\n",
        "# Apply Standard Scaling\n",
        "yb_df_scaled[scaled_features] = scaler.fit_transform(yb_df_scaled[scaled_features])\n",
        "\n",
        "# Check mean and std after scaling\n",
        "print(yb_df_scaled[scaled_features].describe().T)\n",
        "\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "Standardization was used to scale the data.features (e.g., Open, Low, Price Change) have different ranges, so standardization ensures all features contribute equally to the model. Year was scaled  using  minmax scaler.\n",
        "\n",
        "Useful for models sensitive to feature scales.\n",
        "\n",
        "Algorithms like linear regression, logistic regression, SVM, K-means, and PCA perform better when data is standardized.\n",
        "\n"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have only 5 features we dont need any dimensionality reduction."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features and target variable\n",
        "X = yb_df_scaled.drop(columns=['Close'])  # Assuming 'Close' is the target variable\n",
        "y = yb_df_scaled['Close']\n",
        "\n",
        "# Split into training and testing sets (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print shapes of train and test sets\n",
        "print(f\"Train set: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have  used 80-20 splitting ratio\n",
        "Sufficient training data for the model to learn patterns\n",
        "\n",
        "Enough test data to evaluate performance\n",
        "\n",
        "Avoids data leakage by keeping the test set separate"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, dataset is not imbalanced"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yb_df_scaled.isnull().sum()\n",
        "yb_df_scaled.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "3KEgG9zZvPiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yb_df_scaled.head(10)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F8sF8sLr5ypy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = yb_df_scaled.drop(columns=['Close'])  # Drop target variable\n",
        "y = yb_df_scaled['Close']  # Target variable\n",
        "\n",
        "# Split the dataset into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and fit the model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = lr_model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = mse ** 0.5\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"R-squared (R2): {r2:.4f}\")\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_test.values, label=\"Actual Close\", linestyle=\"dashed\", marker=\"o\")\n",
        "plt.plot(y_pred, label=\"Predicted Close\", linestyle=\"dashed\", marker=\"x\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Test Data Points\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.title(\"Actual vs. Predicted Close Price\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "residuals = y_test - y_pred  # Compute residuals (errors)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=y_pred, y=residuals)\n",
        "plt.axhline(y=0, color='r', linestyle='dashed')  # Add zero error line\n",
        "plt.xlabel(\"Predicted Close Price\")\n",
        "plt.ylabel(\"Residuals (Errors)\")\n",
        "plt.title(\"Residual Plot\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rZnigJTBczmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple and interpretable\n",
        "Quick to implement\n",
        "Good for checking linear relationships\n",
        "\n",
        "Cons:Linear Regression model is performing decently, with an R² of 0.8363, meaning it explains about 83.6% of the variance in stock prices. However, the MAE (28.81) and RMSE (38.42) indicate that predictions still have some error.\n",
        "\n"
      ],
      "metadata": {
        "id": "v-B2KzfecCpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    \"alpha\": [0.001, 0.01, 0.1, 1, 10, 100]  # Regularization strength\n",
        "}\n",
        "\n",
        "# Initialize Ridge Regression model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Perform Grid Search with cross-validation (cv=5 means 5-fold cross-validation)\n",
        "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='r2')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model and parameters\n",
        "best_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = mse ** 0.5\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Optimized Ridge Model Performance:\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "n9vB8V4oe704"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression does not include regularization, meaning it can be prone to overfitting if your features have multicollinearity or high variance.\n",
        "Ridge Regression adds L2 regularization, which helps prevent overfitting by penalizing large coefficients."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics\n",
        "models = ['Linear Regression', 'Optimized Ridge Regression']\n",
        "mae_scores = [28.81, 27.43]\n",
        "mse_scores = [1476.28, 1223.53]\n",
        "rmse_scores = [38.42, 34.97]\n",
        "r2_scores = [0.8363, 0.8644]\n",
        "\n",
        "x = np.arange(len(models))  # Model positions\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "\n",
        "# Bar charts for each metric\n",
        "metrics = [mae_scores, mse_scores, rmse_scores, r2_scores]\n",
        "titles = [\"Mean Absolute Error (MAE)\", \"Mean Squared Error (MSE)\",\n",
        "          \"Root Mean Squared Error (RMSE)\", \"R² Score\"]\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.bar(x, metrics[i], color=['blue', 'green'])\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(models, rotation=15)\n",
        "    ax.set_title(titles[i])\n",
        "    ax.set_ylabel(\"Score\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gtyA11d5hXjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rsquare value has increaed from 83 % to 86 % which tells that it now explain more variance. The RMSE has reduced from 38.42 to 34.97 which lowers overall prediction error.There is also a decrease in MSE means less variance in error.(1476.28, 1223.5)"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "# Initialize KNN Regressor\n",
        "knn = KNeighborsRegressor(n_neighbors=9)\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "mae = mean_absolute_error(y_test, y_pred_knn)\n",
        "mse = mean_squared_error(y_test, y_pred_knn)\n",
        "rmse = mse ** 0.5\n",
        "r2 = r2_score(y_test, y_pred_knn)\n",
        "\n",
        "print(f\"KNN Model Performance:\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbors (KNN) is a non-parametric, instance-based learning algorithm. Unlike Linear Regression, which assumes a linear relationship, KNN can capture non-linear patterns in data.predictions are based on actual neighboring points. performs well when data is not too large."
      ],
      "metadata": {
        "id": "MeZOnk42h97R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_test.values, label=\"Actual\", marker='o', linestyle='dashed')\n",
        "plt.plot(y_pred_knn, label=\"Predicted (KNN)\", marker='s', linestyle='dashed', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.xlabel(\"Test Sample Index\")\n",
        "plt.ylabel(\"Close Price (Scaled)\")\n",
        "plt.title(\"Actual vs. Predicted Values (KNN Regression)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals = y_test - y_pred_knn\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=y_test, y=residuals, alpha=0.7)\n",
        "plt.axhline(y=0, color='red', linestyle='--')  # Reference line at 0\n",
        "plt.xlabel(\"Actual Close Price\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residual Plot (KNN Regression)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SX4OLslaREOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KNN model shows varying levels of accuracy in predicting the scaled close price. While some predictions are close to the actual values, there are noticeable deviations, indicating room for improvement.\n",
        "\n",
        "The residual plot suggests that the model may have systematic errors, as the residuals are not entirely random. This could imply that the model is missing some underlying patterns in the data."
      ],
      "metadata": {
        "id": "RyUPQoigS7v0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Define hyperparameter grid\n",
        "param_grid = {'n_neighbors': range(1, 30)}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_knn = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search_knn.fit(X_train, y_train)\n",
        "\n",
        "# Get the best K value\n",
        "best_k = grid_search_knn.best_params_['n_neighbors']\n",
        "print(f\"Best K value: {best_k}\")\n",
        "\n",
        "# Train final model with best K\n",
        "best_knn = KNeighborsRegressor(n_neighbors=best_k)\n",
        "best_knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_best_knn = best_knn.predict(X_test)\n",
        "\n",
        "# Evaluate the optimized model\n",
        "mae = mean_absolute_error(y_test, y_pred_best_knn)\n",
        "mse = mean_squared_error(y_test, y_pred_best_knn)\n",
        "rmse = mse ** 0.5\n",
        "r2 = r2_score(y_test, y_pred_best_knn)\n",
        "\n",
        "print(f\"Optimized KNN Model Performance:\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV for hyperparameter optimization.\n",
        "It systematically searches for the best hyperparameter (in this case, the best value of n_neighbors for KNN) by testing different values from a predefined range (1 to 30).\n",
        "It uses cross-validation (cv=5) to ensure that the model generalizes well and is not overfitting to one specific train-test split.\n",
        "The performance metric used for evaluation is negative mean absolute error (neg_mean_absolute_error) to minimize the absolute errors in stock price predictions.\n",
        "It automatically finds the best K value that minimizes prediction erro"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n"
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN (K=5) is better than the optimized KNN (K=2) because it has lower error metrics (MAE, MSE, RMSE) and a higher R² score.\n",
        "The optimized model (K=2) likely overfits due to choosing a very low K value.\n",
        "The default K=5 is more stable and generalizes better"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAE Directly impacts traders and financial analysts as it shows how far, on average, the model is from the actual stock price.\n",
        "MSE Helps in risk assessment; large errors can lead to big financial losses in trading decisions.\n",
        "RMSE More interpretable for business users, showing the typical magnitude of errors.\n",
        "A low MAE and RMSE indicate the model can accurately predict stock prices, reducing financial risks for investors.\n",
        "A high R² score (0.9756) suggests strong predictive power, meaning traders can trust the model's stock price forecasts to make informed investment decisions.\n",
        "An optimized KNN model allows financial firms to make data-driven trading decisions, improving portfolio performance and minimizing losses"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "\n",
        "# Step 1: Load Data\n",
        "df = yb_df_scaled.copy()  # Ensure dataset is already preprocessed\n",
        "df.index = pd.to_datetime(df.index)\n",
        "# Step 2: Check for Stationarity\n",
        "def adf_test(series, title=\"\"):\n",
        "    result = adfuller(series.dropna())  # Drop NaN values before ADF test\n",
        "    print(f'\\n{title}')\n",
        "    print(f'ADF Statistic: {result[0]}')\n",
        "    print(f'p-value: {result[1]}')\n",
        "    if result[1] > 0.05:\n",
        "        print(\"Series is NOT stationary. Differencing is needed.\")\n",
        "    else:\n",
        "        print(\"Series is stationary.\")\n",
        "\n",
        "# Apply ADF Test on Original Data\n",
        "adf_test(df['Close'], \"ADF Test on Original Series\")\n",
        "\n",
        "# Step 3: First Differencing\n",
        "df['Close_diff1'] = df['Close'].diff()\n",
        "adf_test(df['Close_diff1'], \"ADF Test After First Differencing\")\n",
        "\n",
        "# Step 3.1: Second Differencing\n",
        "df['Close_diff2'] = df['Close'].diff().diff()\n",
        "adf_test(df['Close_diff2'], \"ADF Test After Second Differencing\")\n",
        "\n",
        "# Drop NaN values created by differencing\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Step 4: Plot ACF and PACF to Find p, d, q\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "plot_acf(df['Close_diff2'], ax=ax[0])  # ACF for MA(q)\n",
        "plot_pacf(df['Close_diff2'], ax=ax[1])  # PACF for AR(p)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Fit ARIMA Model with d=2\n",
        "model = ARIMA(df['Close'], order=(1,2,1))  # ARIMA(p=1, d=2, q=1)\n",
        "arima_result = model.fit()\n",
        "\n",
        "# Step 6: Make Predictions\n",
        "df['ARIMA_Predictions'] = arima_result.predict(start=len(df)-50, end=len(df)-1, dynamic=False)\n",
        "\n",
        "# Step 7: Evaluate Model\n",
        "mae = mean_absolute_error(df['Close'][-50:], df['ARIMA_Predictions'][-50:])\n",
        "mse = mean_squared_error(df['Close'][-50:], df['ARIMA_Predictions'][-50:])\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"\\n🔹 ARIMA(1,2,1) Model Performance:\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "\n",
        "# Step 8: Plot Actual vs Predicted Values\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df.index[-100:], df['Close'][-100:], label='Actual', color='blue')\n",
        "plt.plot(df.index[-50:], df['ARIMA_Predictions'][-50:], label='Predicted', color='red')\n",
        "plt.legend()\n",
        "plt.title(\"ARIMA Model - Actual vs Predicted Stock Prices (d=2)\")\n",
        "plt.show()\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "metrics = ['MAE', 'MSE', 'RMSE']\n",
        "values = [23.0863, 1333.3599, 36.5152]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(metrics, values, color=['blue', 'orange', 'green'])\n",
        "plt.ylabel(\"Error Value\")\n",
        "plt.title(\"ARIMA Model - Evaluation Metrics\")\n",
        "plt.yscale(\"log\")  # Use log scale if MSE is significantly larger\n",
        "for i, v in enumerate(values):\n",
        "    plt.text(i, v + 5, f\"{v:.2f}\", ha='center', fontsize=12)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used an ARIMA(p,d,q) model, which is designed for time series forecasting by incorporating autoregression (AR), differencing (I), and moving average (MA) components.\n",
        "\n",
        "p (AutoRegressive Order): Number of past observations used for prediction.\n",
        "d (Differencing Order): Number of times the series is differenced to make it stationary.\n",
        "q (Moving Average Order): Number of past error terms included.\n",
        "\n",
        "The MAE (23.09) suggests that on average, predictions deviate from actual values by about 23 units.\n",
        "\n",
        "The MSE (1333.36) indicates some large deviations, as MSE penalizes large errors more.\n",
        "\n",
        "The RMSE (36.51) is relatively high, suggesting the model can still be optimized.\n"
      ],
      "metadata": {
        "id": "H6j5IA4JbmDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "yLUYThOexRlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "\n",
        "# Install pmdarima if not available\n",
        "!pip install pmdarima\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import pmdarima as pm\n",
        "\n",
        "# Step 1: Load Data\n",
        "df = yb_df_scaled.copy()\n",
        "df.index = pd.to_datetime(df.index)\n",
        "\n",
        "# Step 2: Check Stationarity\n",
        "def adf_test(series, title=\"\"):\n",
        "    result = adfuller(series.dropna())\n",
        "    print(f'\\n{title}')\n",
        "    print(f'ADF Statistic: {result[0]}')\n",
        "    print(f'p-value: {result[1]}')\n",
        "    if result[1] > 0.05:\n",
        "        print(\"Series is NOT stationary. Differencing is needed.\")\n",
        "    else:\n",
        "        print(\"Series is stationary.\")\n",
        "\n",
        "adf_test(df['Close'], \"ADF Test on Original Series\")\n",
        "\n",
        "# Step 3: Differencing\n",
        "df['Close_diff1'] = df['Close'].diff()\n",
        "adf_test(df['Close_diff1'], \"ADF Test After First Differencing\")\n",
        "\n",
        "df['Close_diff2'] = df['Close'].diff().diff()\n",
        "adf_test(df['Close_diff2'], \"ADF Test After Second Differencing\")\n",
        "\n",
        "# Drop NaN values created by differencing\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Step 4: Plot ACF and PACF to help find p and q\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "plot_acf(df['Close_diff2'], ax=ax[0])\n",
        "plot_pacf(df['Close_diff2'], ax=ax[1])\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Automatically Find Best ARIMA Order (p,d,q)\n",
        "print(\"\\n🔹 Finding optimal ARIMA parameters...\")\n",
        "auto_model = pm.auto_arima(\n",
        "    df['Close'],\n",
        "    seasonal=False,\n",
        "    stepwise=True,\n",
        "    trace=True,\n",
        "    suppress_warnings=True\n",
        ")\n",
        "\n",
        "best_order = auto_model.order\n",
        "print(f\"\\n✅ Best ARIMA Order Found: {best_order}\")\n",
        "\n",
        "# Step 6: Fit ARIMA Model with Optimized Parameters\n",
        "model = ARIMA(df['Close'], order=best_order)\n",
        "arima_result = model.fit()\n",
        "\n",
        "# Step 7: Make Predictions\n",
        "df['ARIMA_Predictions'] = arima_result.predict(start=len(df)-50, end=len(df)-1, dynamic=False)\n",
        "\n",
        "# Step 8: Evaluate Model\n",
        "mae = mean_absolute_error(df['Close'][-50:], df['ARIMA_Predictions'][-50:])\n",
        "mse = mean_squared_error(df['Close'][-50:], df['ARIMA_Predictions'][-50:])\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"\\n🔹 Optimized ARIMA Model Performance:\")\n",
        "print(f\"Best Order: {best_order}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "\n",
        "# Step 9: Plot Actual vs Predicted Values\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(df.index[-100:], df['Close'][-100:], label='Actual', color='blue')\n",
        "plt.plot(df.index[-50:], df['ARIMA_Predictions'][-50:], label='Predicted', color='red')\n",
        "plt.legend()\n",
        "plt.title(f\"Optimized ARIMA Model - Actual vs Predicted (Order={best_order})\")\n",
        "plt.show()\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For optimizing the ARIMA model, I used grid search-based hyperparameter tuning with the auto_arima function from the pmdarima library. This technique systematically tests different combinations of ARIMA parameters (p, d, q) to find the best model based on the AIC.\n",
        "\n",
        "ARIMA requires manual selection of p, d, q, which can be complex.\n",
        "Auto-ARIMA automates this process by iterating over multiple values and choosing the best combination that minimizes AIC.\n",
        "Ensures an optimal balance between model complexity and accuracy.\n"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after hyperparameter tuning, I found that the optimized ARIMA model (1,1,1) performed slightly better than the initial model (2,1,2) in terms of AIC and error metrics.There was no significant improvement in MAE, MSE, or RMSE.\n",
        "\n",
        "metrics for  Optimized ARIMA\n",
        "\n",
        "MAE: 22.3016\n",
        "MSE: 1233.7029\n",
        "RMSE: 35.1241   \n",
        "\n",
        "Metrics of ARIMA\n",
        "MAE: 23.0863\n",
        "MSE: 1333.3599\n",
        "RMSE: 36.5152"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the goal is to predict the monthly closing stock price, the evaluation metrics were chosen based on accuracy and business relevance.\n",
        "\n",
        "Mean Absolute Error (MAE) : Measures the average absolute difference between actual and predicted prices. Lower MAE means better performance, as smaller errors indicate more precise forecasts.\n",
        "\n",
        "Mean Squared Error (MSE): Penalizes large errors more than MAE, making it useful for assessing prediction stability.\n",
        "\n",
        "Root Mean Squared Error (RMSE): Helps interpret errors in the same unit as stock prices, making it intuitive for business stakeholders.\n",
        "\n",
        "R² Score: Measures how well the model explains variance in stock prices. A higher R² means the model captures more trends and patterns.\n",
        "\n",
        "In a business context, lower MAE/RMSE means more reliable stock price forecasts, helping investors, analysts, and financial planners make informed decisions with minimal risk."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have selected KNN Lowest MAE (11.81) and RMSE (16.93), meaning smaller errors in predicting closing prices.\n",
        "Highest R² Score (0.968), indicating strong predictive power.\n",
        "Better than ARIMA, which works well for time series but was outperformed by KNN in this case."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n",
        "import shap\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "knn = KNeighborsRegressor(n_neighbors=7)\n",
        "knn.fit(X_train, y_train)\n",
        "explainer = shap.KernelExplainer(knn.predict, X_train[:100])  # Using a sample of 100 for efficiency\n",
        "shap_values = explainer.shap_values(X_test[:50])  # Computing SHAP values for test data sample\n",
        "shap.summary_plot(shap_values, X_test[:50], feature_names=X_train.columns)\n"
      ],
      "metadata": {
        "id": "CYVNHiJGb-6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Chosen: KNN (K-Nearest Neighbors)\n",
        "\n",
        "The K-Nearest Neighbors (KNN) regression model predicts the target variable by finding the \"K\" most similar past data points based on selected features and averaging their values.\n",
        "For this task, I optimized K=7, balancing bias and variance while ensuring robust performance.\n",
        "\n",
        "Works well with non-linear patterns in stock price data.\n",
        "Captures local trends effectively without assuming a strict functional form.\n",
        "Performs well with small-to-medium-sized datasets.\n",
        "\n",
        "Feature Importance Analysis:\n",
        "Earlier i have ussed shapon the KNN to find the importane of each features.\n",
        "\n",
        "Low Price & Open Price -Have the highest impact on predictions. A higher \"Low\" or \"Open\" value tends to increase predictions.\n",
        "\n",
        "Close_lag1 (Previous Day Close Price) -Captures past trends and affects predictions significantly.\n",
        "\n",
        "Rolling Mean_7 (7-day Moving Average) - Helps the model capture short-term trends and smooth price fluctuations.\n",
        "\n",
        "Seasonality Features (Month_cos, Month_sin)\n",
        "\n",
        "These help account for cyclic trends in stock price behavior over time.\n",
        "While they have lower SHAP values, they contribute to minor seasonal adjustments in predictions.\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This analysis aimed to predict Yes Bank’s monthly closing stock price using various machine learning models, with a focus on accuracy and business relevance. After evaluating multiple models, K-Nearest Neighbors (KNN) regression emerged as the best-performing model, outperforming ARIMA in this case.The high accuracy of the KNN model makes it a reliable tool for forecasting Yes Bank’s stock price, enabling investors, analysts, and financial planners to make informed decisions while minimizing risk. The feature importance analysis also provides insights into key factors affecting stock movements, which can aid in refining investment strategies.\n",
        "\n",
        "For future improvements, incorporating market sentiment analysis or testing deep learning models (like LSTMs) could enhance predictive performance further."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}
